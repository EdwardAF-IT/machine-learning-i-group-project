{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "***\n",
    "# Classification\n",
    "MSDS 7331-407, Lab 2 \n",
    "*Jenna Ford, Edward Fry, Christian Nava, and Jonathan Tan* \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Table of Contents\n",
    "\n",
    "<a href='#Section_1'> 1. Introduction </a>  \n",
    "<a href='#Section_2'> 2. Preparation and Dataset Loading </a>  \n",
    "<a href='#Section_3'> 3. Data Preparation Part 1 </a> \n",
    "<a href='#Section_4'> 4. Data Preparation Part 2 </a>  \n",
    "<a href='#Section_5'> 5. Modeling and Evaluation 1 </a>  \n",
    "<a href='#Section_6'> 6. Modeling and Evaluation 2 </a>  \n",
    "<a href='#Section_7'> 7. Modeling and Evaluation 3 </a>  \n",
    "<a href='#Section_7_a'> &nbsp;&nbsp;&nbsp;&nbsp; a. Arrest Type Code </a>  \n",
    "<a href='#Section_7_b'> &nbsp;&nbsp;&nbsp;&nbsp; b. Descent Code </a>  \n",
    "<a href='#Section_8'> 8. Modeling and Evaluation 4 </a>  \n",
    "<a href='#Section_9'> 9. Modeling and Evaluation 5 </a>  \n",
    "<a href='#Section_10'> 10. Modeling and Evaluation 6 </a>  \n",
    "<a href='#Section_11'> 11. Deployment </a>  \n",
    "<a href='#Section_12'> 12. Exceptional Work </a>  \n",
    "\n",
    "<!-- <a href='#Section_4_c_i'> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; i. Cross Street </a>    --> "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<a id = 'Section_1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Introduction\n",
    "\n",
    "For this project our group will continue to use the Los Angeles arrest incidents dataset with arrest incidents dating back to 2010, which can be found [here](https://data.lacity.org/A-Safe-City/Arrest-Data-from-2010-to-Present/yru6-6re4). This dataset contains information about the date, time and location of the incident, demographic data of the person arrested, and information about the type of incident. \n",
    "\n",
    "We will be classifying the variables `Arrest Type` (Felony, Misdemeanor, Infraction, Other) and `Descent Code` (Black, Hispanic, White, Other). We feel that being able to classify arrest type could help in prioritizing dispatch calls, especially when there are more calls than officers available to respond. We assume that arrest type is an indicator of the severity of the incident. It is also worth noting that most of the data available for the classification would be available to the dispatcher. For example, someone placing a call to 911 would most likely have a basic description of the offender such as gender, approximate age and ethnicity. The caller would also know the location of the incident and have a brief description of the situation. \n",
    "\n",
    "As far as classifying `Descent Code`, we are not trying to make any political statements. However, if this variable can be predicted, what can that tell us? Any information discovered from this could be invetigated further. The information could be used for community outreach programs to help guide and direct efforts where they will make the most impact.\n",
    "\n",
    "We 10-fold cross validation to test a variety of models. We also adjust model parameters to fine-tune our classification models. \n",
    "\n",
    "Model effectiveness will be measured using accuracy. A successful model would ideally exceed the ability of a dispatcher to correctly identify the arrest type. However, we do not have data available for this. To be considered effective, the model should significantly outperform random chance (25%), so we will use accuracy of 75% as the cut-off for identifying if the model is successful.\n",
    "\n",
    "NEED TO WORK ON MORE AFTER MODELS AND EVALUATION METRICS ARE DETERMINED"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<a id = 'Section_2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Preparation and Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time the run\n",
    "import time\n",
    "startTime = time.time()\n",
    "\n",
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Creating training and test sets\n",
    "import sklearn\n",
    "\n",
    "# File system management\n",
    "import os.path\n",
    "from multiprocessing import Process\n",
    "processes = []\n",
    "\n",
    "# Supress Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#training/test split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold, ShuffleSplit\n",
    "\n",
    "# run logistic regression and vary some parameters\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# here we can change some of the parameters interactively\n",
    "from ipywidgets import widgets as wd\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "\n",
    "#for weights standardization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, mean_squared_error, classification_report\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Support vector machines\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import plotly\n",
    "plotly.offline.init_notebook_mode() # run at the start of every notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    "  table {margin-left: 0 !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "DATAPATH_BASE = 'https://machinelearningi.blob.core.windows.net/group-project/'\n",
    "DATAPATH_SAS_TOKEN = '?sv=2019-02-02&ss=bfqt&srt=sco&sp=rwdlacup&se=2020-04-27T11:12:37Z&st=2020-01-23T04:12:37Z&spr=https&sig=jpIpjrp8dIg9eyUyPpmgTe5yj9i1ZoCSru5kBVHcUO8%3D'\n",
    "DATAPATH_FILENAME = 'Arrest_Data_from_2010_to_Present.csv'\n",
    "DATAPATH_SMALL_FILENAME = 'Arrest_Data_from_2010_to_Present_Small.csv'\n",
    "\n",
    "# Performance controls\n",
    "SUB_SAMPLE_SIZE = 0.05  #subsample of dataframe\n",
    "SVM_SUB_SAMPLE_SIZE = .05 #subsample for SVM, don't go past 15% of the data because it won't run\n",
    "RUN_SVM1 = False\n",
    "RUN_SVM2 = False\n",
    "RUN_SVM3 = False\n",
    "RUN_SVM4 = False\n",
    "RUN_SVM5 = False\n",
    "RUN_SVM6 = False\n",
    "RUN_SVM7 = False\n",
    "RUN_SVM8 = False\n",
    "RUN_RF1 = False\n",
    "RUN_RF2 = False\n",
    "RUN_RF3 = False\n",
    "RUN_RF4 = False\n",
    "RUN_GRIDSEARCH = False\n",
    "RUN_MAIN1 = False\n",
    "RUN_MAIN2 = False\n",
    "\n",
    "# Fully qualified paths ready to use\n",
    "DATA_SOURCE = \"\".join([DATAPATH_BASE, DATAPATH_FILENAME, DATAPATH_SAS_TOKEN])\n",
    "#DATA_SOURCE = '../Arrest_Data_from_2010_to_Present.csv'\n",
    "\n",
    "# Options\n",
    "pd.set_option('float_format', '{:.2f}'.format)  # Reign in the scientific notation for reasonable values\n",
    "\n",
    "# Load data for analysis; only read if needed because the import can take a long time\n",
    "try:\n",
    "    if len(df.index) < 1:\n",
    "        df_raw = pd.read_csv(DATA_SOURCE) # If we get here, the dataframe was empty\n",
    "except:   \n",
    "    df_raw = pd.read_csv(DATA_SOURCE) # If we get here, the dataframe did not exist\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_raw\n",
    "\n",
    "print(\"The dataset has {:,} rows and {:,} columns\".format(*df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<a id = 'Section_3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. Data Preparation Part 1\n",
    "\n",
    "*(10 points)*  \n",
    "\n",
    "*Define and prepare your class variables. Use proper variable representations (int, float, one-hot, etc.). Use pre-processing methods (as needed) for dimensionality reduction, scaling, etc. Remove variables that are not needed/useful for the analysis.*"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The code below is from Lab 1 where we prepared the initial dataset. Here is a summary of the steps in the code below:\n",
    "* Convert `Time` to a time format and treat missing values and a value of 24:00 as 00:00.\n",
    "* Drop observations where the age of the individual is under 16. This is due to data issues (probably data entry errors).\n",
    "* Drop observations where the `Arrest Type Code` is Dependent. The majority of these observations are for individuals aged 15 or less (and have already been removed from the dataset). There are, however, some remaining observations that we believe to be data entry errors.\n",
    "* Reduce the `Descent Code` classifications to B-Black, H-Hispanic and W-White. Any other ethnicities are grouped into O-Other.\n",
    "* Create an `Hour` variable because knowing the hour and minute an incident occurred is not critical. A window of time will be more appropriate.\n",
    "* Create variables for the day of week and month an incident occurred on. Knowing the exact date an incident occurred is not helpful for future classification, but day of week and month may be.\n",
    "* Drop variables that are not useful for the analysis such as `Location` which has GIS coordinates and description variables that have an associated code variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time - filter out 0 and missing\n",
    "df = df[df['Time'] != 0]\n",
    "df['Time'] = df['Time'].astype(str) \n",
    "df = df[df['Time'] != 'nan']\n",
    "\n",
    "# Time - Convert float to string. Get rid of decimals. Replace missing or invalid values with '0000'.\n",
    "df['Time'] = df['Time'].astype(str).str.split(\".\", expand = True)[0].replace(to_replace = ['2400','nan'], value = '0000') \n",
    "\n",
    "# Time - Fill time column with leading zeros to have 4 characters total\n",
    "df['Time'] = df['Time'].apply(lambda x: '{0:0>4}'.format(x))\n",
    "\n",
    "# Time - Add colon to Time values by converting attribute to a datetime variable \n",
    "df['Time'] = pd.to_datetime(df['Time'], format = '%H%M').dt.time\n",
    "\n",
    "# Charge Group Code - filter out missing values\n",
    "df = df.loc[df['Charge Group Code'].notnull()]\n",
    "\n",
    "# Age - Drop the observations where Age is less than 16\n",
    "df.drop(df[df['Age'] < 16].index, inplace = True) \n",
    "\n",
    "# Arrest Type Code - Drop the observations where Arrest Type Code = 'D'\n",
    "df.drop(df[df['Arrest Type Code'] == 'D'].index, inplace = True) \n",
    "\n",
    "# Descent Code - Re-classify any descent not in (B,H,O,W) into 0\n",
    "descent_list = ['B','H','O','W']\n",
    "df['Descent Code'] = np.where(np.isin(df['Descent Code'],descent_list),df['Descent Code'],'O')\n",
    "\n",
    "# Get hour\n",
    "df['Hour'] = pd.to_datetime(df['Time'], format='%H:%M:%S').dt.hour\n",
    "\n",
    "# Convert Arrest Date to datetime\n",
    "df['Arrest Date'] = pd.to_datetime(df['Arrest Date'])\n",
    "\n",
    "# Extract month, and day of week and add to dataframe as new attributes\n",
    "df['arrest_month']= df['Arrest Date'].dt.month\n",
    "try:\n",
    "    df['arrest_day_of_week'] = df['Arrest Date'].dt.weekday_name\n",
    "except:\n",
    "    df['arrest_day_of_week'] = 'monday'\n",
    "\n",
    "# remove unecessary columns\n",
    "df.drop(['Cross Street','Charge Description','Charge','Charge Group Description','Time',\n",
    "         'Arrest Date','Report ID','Address','Area Name','Location'], axis=1, inplace=True)\n",
    "\n",
    "# Change data types\n",
    "df['Age'] = df['Age'].astype(np.int8)\n",
    "df['Reporting District'] = df['Reporting District'].astype(np.str)\n",
    "df['Area ID'] = df['Area ID'].astype(np.str)\n",
    "df['Charge Group Code'] = df['Charge Group Code'].astype(np.str)\n",
    "df['Hour'] = df['Hour'].astype(np.str)\n",
    "df['arrest_month'] = df['arrest_month'].astype(np.str)\n",
    "df['arrest_day_of_week'] = df['arrest_day_of_week'].astype(np.str)\n",
    "\n",
    "df_lightgbm = df\n",
    "# print clean dataset\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The code below prepares the dataset for classification tasks. First, we group `Age` into bins of 10 years and perform integer encoding, since the data is ordinal. Next, we perform one-hot encoding on all categorical variables. We use one-hot encoding here because no natural ordering exists in the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create buckets for Age\n",
    "df['age_range'] = pd.cut(df.Age,[15,25,35,45,55,65,75,1e6],4,labels=[0,1,2,3,4,5,6]) # this creates a new variable\n",
    "df['age_range'] = df['age_range'].fillna(0)\n",
    "df['age_range'] = df.age_range.astype(np.int)\n",
    "\n",
    "# Replace the current Sex atribute with something slightly more intuitive and readable\n",
    "df['IsMale'] = df['Sex Code']=='M' \n",
    "df.IsMale = df.IsMale.astype(np.int)\n",
    "\n",
    "# Perform one-hot encoding of the categorical data \"DOW\"\n",
    "tmp_df = pd.get_dummies(df['arrest_day_of_week'],prefix='DOW',drop_first=True)\n",
    "df = pd.concat((df,tmp_df),axis=1) # add back into the dataframe\n",
    "\n",
    "# Perform one-hot encoding of the categorical data \"Area ID\"\n",
    "tmp_df = pd.get_dummies(df['Area ID'],prefix='Area',drop_first=True)\n",
    "df = pd.concat((df,tmp_df),axis=1) # add back into the dataframe\n",
    "\n",
    "# Perform one-hot encoding of the categorical data \"Charge Group Code\"\n",
    "tmp_df = pd.get_dummies(df['Charge Group Code'],prefix='Charge',drop_first=True)\n",
    "df = pd.concat((df,tmp_df),axis=1) # add back into the dataframe\n",
    "\n",
    "# Perform one-hot encoding of the categorical data \"Hour\"\n",
    "tmp_df = pd.get_dummies(df['Hour'],prefix='Hour',drop_first=True)\n",
    "df = pd.concat((df,tmp_df),axis=1) # add back into the dataframe\n",
    "\n",
    "# Perform one-hot encoding of the categorical data \"Month\"\n",
    "tmp_df = pd.get_dummies(df['arrest_month'],prefix='Month',drop_first=True)\n",
    "df = pd.concat((df,tmp_df),axis=1) # add back into the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Since we have 2 classification tasks (`Arrest Type Code` and `Descent Code`), we need 2 datasets. The dataset for classifying `Arrest Type Code` will need to have `Descent Code` one-hot encoded and the dataset for classifying `Descent Code` will need to have `Arrest Type Code` one-hot encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arrest = df\n",
    "df_descent = df\n",
    "\n",
    "#Final encoding steps for Arrest Type Code classification dataset\n",
    "# Encode Arrest Type Code as Categorical\n",
    "cleanup_arrest = {\"Arrest Type Code\": {\"F\": 0, \"M\": 1, \"I\": 2, \"O\":3}}\n",
    "df_arrest.replace(cleanup_arrest,inplace=True)\n",
    "\n",
    "# Perform one-hot encoding of the categorical data \"Descent Code\"\n",
    "tmp_df = pd.get_dummies(df_arrest['Descent Code'],prefix='Descent',drop_first=True)\n",
    "df_arrest = pd.concat((df_arrest,tmp_df),axis=1) # add back into the dataframe\n",
    "\n",
    "df_arrest.drop(['Sex Code','Descent Code','arrest_day_of_week','Area ID','Reporting District','Charge Group Code',\n",
    "         'Age','Hour','arrest_month'], axis=1, inplace=True)\n",
    "\n",
    "df_arrest.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final encoding steps for Descent Code classification dataset\n",
    "cleanup_descent = {\"Descent Code\": {\"B\": 0, \"H\": 1, \"W\": 2, \"O\":3}}\n",
    "df_descent.replace(cleanup_descent,inplace=True)\n",
    "\n",
    "# Perform one-hot encoding of the categorical data \"Arrest Type Code\"\n",
    "tmp_df = pd.get_dummies(df_descent['Arrest Type Code'],prefix='Arrest',drop_first=True)\n",
    "df_descent = pd.concat((df_descent,tmp_df),axis=1) #add back to the dataframe\n",
    "\n",
    "df_descent.drop(['Sex Code','Arrest Type Code','arrest_day_of_week','Area ID','Reporting District','Charge Group Code',\n",
    "         'Age','Hour','arrest_month'], axis=1, inplace=True)\n",
    "\n",
    "df_descent.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The Arrest classification dataset set has {:,} rows and {:,} columns\".format(*df_arrest.shape))\n",
    "print(\"The Descent classification dataset set has {:,} rows and {:,} columns\".format(*df_descent.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<a id = 'Section_4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4. Data Preparation Part 2\n",
    "\n",
    "*(5 points)*\n",
    "\n",
    "*Describe the final dataset that is used for classification/regression (include a description of any newly formed variables you created).*"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Table 1: Description of Final Dataset\n",
    "\n",
    "| Dataset | Target/Explanatory| Variable Prefix | Variable Suffix Range | Description |\n",
    "| :-- | :-- | :-- | :-- | :-- |\n",
    "| Arrest | Target| Arrest Type Code | NA | Felony=0, Misdemeanor=1, Infraction=2, Other=3 |  \n",
    "| Descent | Target| Descent Code | NA | Black=0, Hispanic=1, White=2, Other=3 | \n",
    "| Both | Explanatory| age_range | NA | Age bins: 16-25=0, 25-35=1, 35-45=2, 45-55=3, 55-65=4, 65-75=5, 75+=6|\n",
    "| Both | Explanatory| IsMale | NA | 0=Female, 1=Male|\n",
    "| Both | Explanatory| DOW | Saturday-Thursday | Value of 1 indicates the DOW the incident occurred on. DOW_Friday is the comparison and was dropped from the dataset.|\n",
    "| Both | Explanatory| Area | 2-21 | Value of 1 indicates the Area the incident occurred in. Area_1 is the comparison and was dropped from the dataset.|\n",
    "| Both | Explanatory| Charge | 2-29, 99, nan | Value of 1 indicates the Charge relating to the incident. Charge_1 is the comparison and was dropped from the dataset.|\n",
    "| Both | Explanatory| Hour | 1-23 | Value of 1 indicates the Hour the incident occurred. Hour_0 (00:00-00:59 on the 24-hour clock) is the comparison and was dropped from the dataset.|\n",
    "| Both | Explanatory| Month | 2-12 | Value of 1 indicates the month the incident occurred. Month_1 (January) is the comparison and was dropped from the dataset.|\n",
    "| Arrest | Explanatory| Descent | H,W,O | Value of 1 indicates the descent of the individual. Descent_H: Hispanic, Descent_W: White and Descent_O: Other. Descent_B (Black) is the comparison and was dropped from the dataset.|\n",
    "| Descent | Explanatory| Arrest | 1-3 | Value of 1 indicates the type of arrest made. Arrest_1: Misdemeanor, Arrest2: Infraction and Arrest_3: Other. Arrest_0 (Felony) is the comparison and was dropped from the dataset.|"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<a id = 'Section_5'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5. Modeling and Evaluation 1\n",
    "\n",
    "*(10 points)*\n",
    "\n",
    "*Choose and explain your evaluation metrics that you will use (i.e., accuracy, precision, recall, F-measure, or any metric we have discussed). Why are the measure(s) appropriate for analyzing the results of your modeling? Give a detailed explanation backing up any assertions.*"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The metric used for this project is the area under the receiver operating characteristic curve (AUC). The receiver operating characteristic curve (ROC) graphs the performance of a classification model at all classification thresholds; it plots the true positive rate (TPR) against the false positive rate (FPR) as shown in Figure 1.\n",
    "\n",
    "<h4 align=\"center\">Figure 1: Area Under the Reciever Operating Curve</h4> \n",
    "<img src=\"https://marlin-prod.literatumonline.com/cms/attachment/34661288-1f8f-459e-b8b4-936efc49e9bc/fx1_lrg.jpg\" style=\"width:350px;height:300px\"/>\n",
    "\n",
    "Source: [Journal of Thoracic and Cardiovascular Surgery](https://www.jtcvs.org/article/S0022-5223(18)32875-7/fulltext)\n",
    "\n",
    "Each curve represents the graph of a single model. Movement along the curve indicates changing the threshold used for classifying a positive instance. The area under the ROC curve measures the two-dimensional area under the ROC curve and is given by the integral of the curve, or the area under the curve (AUC): $$AUC = \\int_{0}^{1}TPR(x)dx = \\int_{0}^{1}P(A>\\tau (x))dx$$\n",
    "\n",
    "where $x$ is the false positive rate and $A$ is the distribution of scores the model produces for data points that are actually in the positive class.\n",
    "\n",
    "This metric is measured between 0 and 1. A higher score indicates a better model. Visually, a superior model will graph as a curve above and to the left of another curve. A straight diagonal line beginning at 0 on the bottom left and ending at 1 on the top right indicates a naive model (random guessing). Other metrics such as accuracy are not the best metric for data with imbalanced classes. The Los Angeles City Arrest dataset is imbalanced. \n",
    "\n",
    "A model with a high ROC AUC will also have a high accuracy, therefore, the AUC score will be the metric employed in this analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<a id = 'Section_6'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 6. Modeling and Evaluation 2\n",
    "\n",
    "*(10 points)*\n",
    "\n",
    "*Choose the method you will use for dividing your data into training and testing splits (i.e., are you using Stratified 10-fold cross validation? Why?). Explain why your chosen method is appropriate or use more than one method as appropriate. For example, if you are using time series data then you should be using continuous training and testing sets across time.*\n",
    "\n",
    "Cross validation allows us to test our model on data it hasn't seen, to see how well the model performs when making predictions. Using cross validation can help identify if the model is overfitting the training data. If the model is overfitting the training data we would expect to see low evaluation metrics when predictions are created for the test dataset.\n",
    "\n",
    "We use stratified 10-fold cross validation for this project. The classifiers we are using for this project are unbalanced. The first table below shows that 61% of arrests are Misdemeanors and only 2% of arrests are identified as Other. The second table below shows that 46% of arrests are for Hispanic individuals and 6% of arrests are for individuals grouped into an Other ethnicity bin. Balanced data would have approximately 25% of all arrests in each bucket (for both classifiers)."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<a id = 'Section_7'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a function to count by column and display percentages\n",
    "def count_percent(data,field):\n",
    "    df_grouped = data.groupby(by=field)\n",
    "    c1 = (df_grouped[field].count())\n",
    "\n",
    "    c2  = []#create empty list to store percentages\n",
    "    for x in c1:\n",
    "        c2.append(\"{:.2%}\".format((x/sum(c1)))) #row value divided by total, formatted as percent, store in list\n",
    "    c1 = pd.DataFrame(c1) #needs dataframe to start with, then can add new column from list c2\n",
    "    c1['Percent']= c2\n",
    "    c1.columns = ['Count','Percent']\n",
    "    return c1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_percent(df,'Arrest Type Code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_percent(df,'Descent Code')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Due to having extremely unbalanced data it is important for us to use a cross validation technique that stratifies the sample, matching the same percentages of the classifier from the population dataset. This is the 'stratified' part of the stratified 10-fold cross validation tecnhique we have chosen. Without stratifying the folds we cannot ensure that the folds are representative of the population; leading to high bias and variance.\n",
    "\n",
    "When splitting data into training and test datasets there is a concern that important patterns found in the data may be left out of the training dataset. If these patterns are not present in the training dataset the model won't be able to predict them in the test dataset. Using k-fold cross validation takes this into account. K-fold cross validation divides the data into k folds. K-1 folds are used to train the model and the remaining fold is used for testing. This is repeated for k-1 models and every fold is used for testing once. This reduces bias and variance since we are essentially using the entire dataset for training and testing. We have chosen 10-folds for our project, a typical value chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<a id = 'Section_7'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 7. Modeling and Evaluation 3\n",
    "\n",
    "*(20 points)*\n",
    "\n",
    "*Create three different classification/regression models for each task (e.g., random forest, KNN, and SVM for task one and the same or different algorithms for task two). Two modeling techniques must be new (but the third could be SVM or logistic regression). Adjust parameters as appropriate to increase generalization performance using your chosen metric. You must investigate different parameters of the algorithms!*"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<a id = 'Section_7_a'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arrest1 = df_arrest\n",
    "df_descent1 = df_descent\n",
    "\n",
    "#subsample of dataframe\n",
    "df_arrest1 = pd.DataFrame.sample(df_arrest1, frac = SUB_SAMPLE_SIZE, random_state = 34128)\n",
    "df_descent1 = pd.DataFrame.sample(df_descent1, frac = SUB_SAMPLE_SIZE, random_state = 34128)\n",
    "\n",
    "print(\"The datasets have {:,} rows and {:,} columns\".format(*df_arrest1.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### a. Arrest Type Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "\n",
    "# create variables we are more familiar with\n",
    "if 'Arrest Type Code' in df_arrest1:\n",
    "    ya = df_arrest1['Arrest Type Code'].values # get the labels we want\n",
    "    del df_arrest1['Arrest Type Code'] # get rid of the class label\n",
    "    Xa = df_arrest1.values # use everything else to predict!\n",
    "    \n",
    "yhata = np.zeros(ya.shape) # we will fill this with predictions\n",
    "\n",
    "scl = StandardScaler()\n",
    "Xa = scl.fit_transform(Xa)\n",
    "\n",
    "# create cross validation iterator\n",
    "cv = StratifiedKFold(n_splits=10, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_class_accuracy(ytrue,yhat):\n",
    "    conf = mt.confusion_matrix(ytrue,yhat)\n",
    "    norm_conf = conf.astype('float') / conf.sum(axis=1)[:, np.newaxis]\n",
    "    return np.diag(norm_conf)\n",
    "\n",
    "def plot_class_acc(ytrue,yhat, title=''):\n",
    "    acc_list = per_class_accuracy(ytrue,yhat)\n",
    "    plt.bar(range(len(acc_list)), acc_list)\n",
    "    plt.xlabel('0-Felony, 1-Misdemeanor, 2-Infraction, 3-Other')\n",
    "    plt.ylabel('Accuracy within class')\n",
    "    plt.title(title+\", Total Acc=%.1f\"%(100*mt.accuracy_score(ytrue,yhat)))\n",
    "    plt.grid()\n",
    "    plt.ylim([0,1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### XGBoost - Arrest Type Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KGBoost \n",
    "import pickle\n",
    "import xgboost as xgb\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, mean_squared_error, classification_report, roc_auc_score\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "def XGBoosta(boost, tree, depth, delta_step, etaparm):\n",
    "    for train, test in cv.split(Xa,ya):\n",
    "        xgb_model = xgb.XGBClassifier(booster=boost, tree_method=tree, max_depth=depth, max_delta_step=delta_step,eta=etaparm).fit(Xa[train], ya[train])\n",
    "        yhata[test] = xgb_model.predict(Xa[test])\n",
    "    \n",
    "    lb=LabelBinarizer()\n",
    "    lb.fit(ya)\n",
    "    ya_lb = lb.transform(ya)\n",
    "    yhata_lb = lb.transform(yhata)\n",
    "    print ('AUC Score', roc_auc_score(ya_lb, yhata_lb, average='weighted'))\n",
    "    \n",
    "    print (mt.classification_report(ya,yhata,digits=3))\n",
    "                \n",
    "    plot_class_acc(ya,yhata,title=\"XGBoost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if RUN_MAIN1:\n",
    "    boost='gbtree'\n",
    "    tree='auto'\n",
    "    depth=6\n",
    "    delta_step=0\n",
    "    etaparm=0.3\n",
    "\n",
    "    XGBoosta(boost,tree,depth,delta_step,etaparm)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Modifying Booster Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if RUN_MAIN1:\n",
    "    boost='gblinear'\n",
    "    tree='auto'\n",
    "    depth=6\n",
    "    delta_step=0\n",
    "    etaparm=0.3\n",
    "\n",
    "    XGBoosta(boost,tree,depth,delta_step,etaparm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if RUN_MAIN1:\n",
    "    boost='dart'\n",
    "    tree='auto'\n",
    "    depth=6\n",
    "    delta_step=0\n",
    "    etaparm=0.3\n",
    "\n",
    "    XGBoosta(boost,tree,depth,delta_step,etaparm)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Modifying Booster & Tree Method Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if RUN_MAIN1:\n",
    "    boost='gbtree'\n",
    "    tree='hist'\n",
    "    depth=6\n",
    "    delta_step=0\n",
    "    etaparm=0.3\n",
    "\n",
    "    XGBoosta(boost,tree,depth,delta_step,etaparm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if RUN_MAIN1:\n",
    "    boost='gblinear'\n",
    "    tree='hist'\n",
    "    depth=6\n",
    "    delta_step=0\n",
    "    etaparm=0.3\n",
    "\n",
    "    XGBoosta(boost,tree,depth,delta_step,etaparm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if RUN_MAIN1:\n",
    "    boost='dart'\n",
    "    tree='hist'\n",
    "    depth=6\n",
    "    delta_step=0\n",
    "    etaparm=0.3\n",
    "\n",
    "    XGBoosta(boost,tree,depth,delta_step,etaparm)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Modifying Booster, Tree Method and Max Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if RUN_MAIN1:\n",
    "    boost='gbtree'\n",
    "    tree='hist'\n",
    "    depth=5\n",
    "    delta_step=0\n",
    "    etaparm=0.3\n",
    "\n",
    "    XGBoosta(boost,tree,depth,delta_step,etaparm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if RUN_MAIN1:\n",
    "    boost='gbtree'\n",
    "    tree='hist'\n",
    "    depth=7\n",
    "    delta_step=0\n",
    "    etaparm=0.3\n",
    "\n",
    "    XGBoosta(boost,tree,depth,delta_step,etaparm)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Modifying Booster, Tree Method and Max Delta Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if RUN_MAIN1:\n",
    "    boost='gbtree'\n",
    "    tree='hist'\n",
    "    depth=6\n",
    "    delta_step=1\n",
    "    etaparm=0.3\n",
    "\n",
    "    XGBoosta(boost,tree,depth,delta_step,etaparm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if RUN_MAIN1:\n",
    "    boost='gbtree'\n",
    "    tree='hist'\n",
    "    depth=6\n",
    "    delta_step=5\n",
    "    etaparm=0.3\n",
    "\n",
    "    XGBoosta(boost,tree,depth,delta_step,etaparm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if RUN_MAIN1:\n",
    "    boost='gbtree'\n",
    "    tree='hist'\n",
    "    depth=6\n",
    "    delta_step=10\n",
    "    etaparm=0.3\n",
    "\n",
    "    XGBoosta(boost,tree,depth,delta_step,etaparm)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### XGBoost Performance - Arrest Type Code"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Table 2: XGBoost Performance for Arrest Type Code\n",
    "Performance metrics for each Arrest Type are listed as *Precision, Recall*.\n",
    "\n",
    "| Booster | Tree Method | Max Depth | Max Delta Step | Time | Accuracy | AUC | Felony | Misdemeanor | Infraction | Other | \n",
    "| :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- |\n",
    "| gbtree | auto | 6 | 0 | | | | | | | |\n",
    "| gblinear | auto | 6 | 0 | | | | | | | |\n",
    "| dart | auto | 6 | 0 |  | | | | | | |\n",
    "| gbtree | hist | 6 | 0 | | | | | | | |\n",
    "| gblinear | hist | 6 | 0 | | | | | | | |\n",
    "| dart | hist | 6 | 0 | | | | | | | |\n",
    "| gbtree | hist | 5 | 0 | | | | | | | |\n",
    "| gbtree | hist | 7 | 0 | | | | | | | |\n",
    "| gbtree | hist | 6 | 1 | | | | | | | |\n",
    "| gbtree | hist | 6 | 5 | | | | | | | |\n",
    "| gbtree | hist | 6 | 10 | | | | | | | |"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LightGBM - Arrest Type Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "def LightGBMa(estimators, l_rate, leaves, r_alpha, r_lambda, depth, subsample, thread, split_gain, colsample):\n",
    "    for train, test in cv.split(Xa,ya):\n",
    "        lgbm_model = lgb.LGBMClassifier(n_estimators = estimators,\n",
    "                                        objective = 'multiclass',\n",
    "                                        learning_rate = l_rate,\n",
    "                                        num_leaves = leaves,\n",
    "                                        lambda_l1 = r_alpha,\n",
    "                                        lambda_l2 = r_lambda, \n",
    "                                        maxdepth = depth,\n",
    "                                        bagging_fraction = subsample,\n",
    "                                        nthread = thread,\n",
    "                                        min_gain_to_split = split_gain,\n",
    "                                        feature_fraction = colsample,\n",
    "                                        random_state = 17).fit(Xa[train], ya[train])\n",
    "        yhata[test] = lgbm_model.predict(Xa[test])\n",
    "    \n",
    "    lb = LabelBinarizer()\n",
    "    lb.fit(ya)\n",
    "    ya_lb = lb.transform(ya)\n",
    "    yhata_lb = lb.transform(yhata)\n",
    "    print ('AUC Score', roc_auc_score(ya_lb, yhata_lb, average = 'weighted'))\n",
    "    \n",
    "    print (mt.classification_report(ya, yhata, digits = 3))\n",
    "                \n",
    "    plot_class_acc(ya, yhata, title = \"LightGBM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if RUN_MAIN1:\n",
    "    estimators = 100\n",
    "    objective = 'multiclass'\n",
    "    l_rate = 0.1\n",
    "    leaves = 31\n",
    "    r_alpha = 0.0\n",
    "    r_lambda = 0.0\n",
    "    depth = -1\n",
    "    subsample = 1.0\n",
    "    thread = 0\n",
    "    split_gain = 0.0\n",
    "    colsample = 1.0\n",
    "\n",
    "\n",
    "    LightGBMa(estimators, l_rate, leaves, r_alpha, r_lambda, depth, subsample, thread, split_gain, colsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if RUN_MAIN1:\n",
    "    estimators = 1000\n",
    "    objective = 'multiclass'\n",
    "    l_rate = 0.05\n",
    "    leaves = 31\n",
    "    r_alpha = 0.0\n",
    "    r_lambda = 0.0\n",
    "    depth = -1\n",
    "    subsample = 1.0\n",
    "    thread = 0\n",
    "    split_gain = 0.0\n",
    "    colsample = 1.0\n",
    "\n",
    "\n",
    "    LightGBMa(estimators, l_rate, leaves, r_alpha, r_lambda, depth, subsample, thread, split_gain, colsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if RUN_MAIN1:\n",
    "    estimators = 10000\n",
    "    objective = 'multiclass'\n",
    "    l_rate = 0.05\n",
    "    leaves = 31\n",
    "    r_alpha = 0.0\n",
    "    r_lambda = 0.0\n",
    "    depth = -1\n",
    "    subsample = 1.0\n",
    "    thread = 0\n",
    "    split_gain = 0.0\n",
    "    colsample = 1.0\n",
    "\n",
    "\n",
    "    LightGBMa(estimators, l_rate, leaves, r_alpha, r_lambda, depth, subsample, thread, split_gain, colsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if RUN_MAIN1:\n",
    "    estimators = 1000\n",
    "    objective = 'multiclass'\n",
    "    l_rate = 0.05\n",
    "    leaves = 31\n",
    "    r_alpha = 0.5\n",
    "    r_lambda = 0.5\n",
    "    depth = 2\n",
    "    subsample = 1.0\n",
    "    thread = 0\n",
    "    split_gain = 0.02\n",
    "    colsample = 0.8\n",
    "\n",
    "\n",
    "    LightGBMa(estimators, l_rate, leaves, r_alpha, r_lambda, depth, subsample, thread, split_gain, colsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if RUN_MAIN1:\n",
    "    estimators = 10000\n",
    "    objective = 'multiclass'\n",
    "    l_rate = 0.05\n",
    "    leaves = 31\n",
    "    r_alpha = 0.5\n",
    "    r_lambda = 0.5\n",
    "    depth = 2\n",
    "    subsample = 1.0\n",
    "    thread = 0\n",
    "    split_gain = 0.02\n",
    "    colsample = 0.8\n",
    "\n",
    "\n",
    "    LightGBMa(estimators, l_rate, leaves, r_alpha, r_lambda, depth, subsample, thread, split_gain, colsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if RUN_MAIN1:\n",
    "    estimators = 10000\n",
    "    objective = 'multiclass'\n",
    "    l_rate = 0.05\n",
    "    leaves = 31\n",
    "    r_alpha = 0.05\n",
    "    r_lambda = 0.05\n",
    "    depth = 7\n",
    "    subsample = 1.0\n",
    "    thread = 0\n",
    "    split_gain = 0.02\n",
    "    colsample = 0.8\n",
    "\n",
    "\n",
    "    LightGBMa(estimators, l_rate, leaves, r_alpha, r_lambda, depth, subsample, thread, split_gain, colsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if RUN_MAIN1:\n",
    "    estimators = 10000\n",
    "    objective = 'multiclass'\n",
    "    l_rate = 0.05\n",
    "    leaves = 34\n",
    "    r_alpha = 0.05\n",
    "    r_lambda = 0.05\n",
    "    depth = 7\n",
    "    subsample = 1.0\n",
    "    thread = 0\n",
    "    split_gain = 0.02\n",
    "    colsample = 0.8\n",
    "\n",
    "\n",
    "    LightGBMa(estimators, l_rate, leaves, r_alpha, r_lambda, depth, subsample, thread, split_gain, colsample)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### LightGBM Performance - Arrest Type Code"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Table 3: LightGBM Performance for Arrest Type Code\n",
    "Performance metrics for each Arrest Type are listed as *Precision, Recall*.\n",
    "\n",
    "| Param | Param | Param | Param | Time | Accuracy | AUC | Felony | Misdemeanor | Infraction | Other | \n",
    "| :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- |\n",
    "| gbtree | auto | 6 | 0 | | | | | | | |\n",
    "| gblinear | auto | 6 | 0 | | | | | | | |\n",
    "| dart | auto | 6 | 0 |  | | | | | | |\n",
    "| gbtree | hist | 6 | 0 | | | | | | | |\n",
    "| gblinear | hist | 6 | 0 | | | | | | | |\n",
    "| dart | hist | 6 | 0 | | | | | | | |\n",
    "| gbtree | hist | 5 | 0 | | | | | | | |\n",
    "| gbtree | hist | 7 | 0 | | | | | | | |\n",
    "| gbtree | hist | 6 | 1 | | | | | | | |\n",
    "| gbtree | hist | 6 | 5 | | | | | | | |\n",
    "| gbtree | hist | 6 | 10 | | | | | | | |"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Random Forest - Arrest Type Code"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Random Forest Performance - Arrest Type Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if RUN_RF1:\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "    maxAccPCARF = 0\n",
    "    optimalEstimatorsPCARF = 0\n",
    "    optimalYPCARF = []\n",
    "    optimalYhatPCARF = []\n",
    "    clReport = ''\n",
    "\n",
    "    for i in range(100, 201, 50):\n",
    "        clf_pipe = Pipeline(\n",
    "            [('PCA',PCA(n_components=50, svd_solver='randomized')),\n",
    "             ('CLF',RandomForestClassifier(n_estimators=i, n_jobs=-1))]\n",
    "        )\n",
    "\n",
    "        # now iterate through and get predictions, saved to the correct row in yhat\n",
    "        for train, test in cv.split(Xa,ya):\n",
    "            clf_pipe.fit(Xa[train],ya[train])\n",
    "            yhata[test] = clf_pipe.predict(Xa[test])\n",
    "\n",
    "        plt.title('Confusion Matrix for {0} Estimators'.format(i), fontsize = 20)\n",
    "        plt.imshow(mt.confusion_matrix(ya, yhata),cmap=plt.get_cmap('Greens'),aspect='auto')\n",
    "        plt.grid(False)\n",
    "        plt.show()\n",
    "\n",
    "        total_accuracy = mt.accuracy_score(ya, yhata)\n",
    "        if total_accuracy > maxAccPCARF:\n",
    "            maxAccPCARF = total_accuracy\n",
    "            optimalEstimatorsPCARF = i\n",
    "            optimalYPCARF = ya\n",
    "            optimalYhatPCARF = yhata\n",
    "            clReport = mt.classification_report(ya,yhata,digits=3)\n",
    "\n",
    "        lb = LabelBinarizer()\n",
    "        lb.fit(ya)\n",
    "        ya_lb = lb.transform(ya)\n",
    "        yhata_lb = lb.transform(yhata)\n",
    "        print (\"Acc: \", total_accuracy)\n",
    "        print (\"AUC: \", roc_auc_score(ya_lb, yhata_lb))\n",
    "\n",
    "    print ('Best accuracy is ', maxAccPCARF, ' with ', optimalEstimatorsPCARF, ' estimators in a random forest with PCA')\n",
    "    print(clReport)\n",
    "    plot_class_acc(optimalYPCARF,optimalYhatPCARF,title=\"Random Forest + PCA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if RUN_RF2:\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    maxAccRF = 0\n",
    "    optimalEstimatorsRF = 0\n",
    "    clReport = ''\n",
    "\n",
    "    for i in range(100, 201, 50):\n",
    "        clf = RandomForestClassifier(n_estimators=i, n_jobs=-1, oob_score=True)\n",
    "\n",
    "        # now iterate through and get predictions, saved to the correct row in yhat\n",
    "        for train, test in cv.split(Xa,ya):\n",
    "            clf.fit(Xa[train],ya[train])\n",
    "            yhata[test] = clf.predict(Xa[test])\n",
    "\n",
    "        total_accuracy = mt.accuracy_score(ya, yhata)\n",
    "\n",
    "        plt.title('Confusion Matrix for {0} estimators'.format(i), fontsize = 20)\n",
    "        plt.imshow(mt.confusion_matrix(ya, yhata),cmap=plt.get_cmap('Greens'),aspect='auto')\n",
    "        plt.grid(False)\n",
    "        plt.show()\n",
    "\n",
    "        if total_accuracy > maxAccRF:\n",
    "            maxAccRF = total_accuracy\n",
    "            optimalEstimatorsRF = i\n",
    "            optimalYRF = ya\n",
    "            optimalYhatRF = yhata\n",
    "            clReport = mt.classification_report(ya,yhata,digits=3)\n",
    "\n",
    "        lb = LabelBinarizer()\n",
    "        lb.fit(ya)\n",
    "        ya_lb = lb.transform(ya)\n",
    "        yhata_lb = lb.transform(yhata)\n",
    "        print (\"Acc: \", total_accuracy)\n",
    "        print (\"AUC: \", roc_auc_score(ya_lb, yhata_lb))\n",
    "\n",
    "    print ('Best accuracy is ', maxAccRF, ' with ', optimalEstimatorsRF, ' Estimators in a Raw Random Forest')\n",
    "    print(clReport)\n",
    "    plot_class_acc(optimalYRF,optimalYhatRF,title=\"Random Forest, Raw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Table 4: Random Forest Performance for Arrest Type Code\n",
    "Performance metrics for each Arrest Type are listed as *Precision, Recall*.\n",
    "\n",
    "| Param | Param | Param | Param | Time | Accuracy | AUC | Felony | Misdemeanor | Infraction | Other | \n",
    "| :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- |\n",
    "| gbtree | auto | 6 | 0 | | | | | | | |\n",
    "| gblinear | auto | 6 | 0 | | | | | | | |\n",
    "| dart | auto | 6 | 0 |  | | | | | | |\n",
    "| gbtree | hist | 6 | 0 | | | | | | | |\n",
    "| gblinear | hist | 6 | 0 | | | | | | | |\n",
    "| dart | hist | 6 | 0 | | | | | | | |\n",
    "| gbtree | hist | 5 | 0 | | | | | | | |\n",
    "| gbtree | hist | 7 | 0 | | | | | | | |\n",
    "| gbtree | hist | 6 | 1 | | | | | | | |\n",
    "| gbtree | hist | 6 | 5 | | | | | | | |\n",
    "| gbtree | hist | 6 | 10 | | | | | | | |"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SVM - Arrest Type Code"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In this section, we will explore support vector machines. First, a variety of SVM kernels are used, and we compare the results to see which ones are the most and least accurate. For the linear kernel, we can take the additional step of looking at the weights and plotting out the most meaningful.  Once we have the most accurate SVM model, which turns out to be RBF, we plot KDEs for all support vectors and compare that to their before state.  Finally, just for fun, we also look at a stochastic gradient descent (SGD) model. Interestingly, the SGD model performs about as well as the top models that don't use SGD.\n",
    "\n",
    "One thing that we noticed right away is just how long it takes to train SVM models. We found that the training time grew exponentially as the size of the dataset grew, and we simply did not have enough time, after days of waiting, to use the full dataset in this section.  As a result, we opted to use a much smaller version of the dataset, which was built by randomly sampling 15% of data in the original dataset.  This resulted in 160,477 observations, which is much more manageable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing the sample size because we can't get the full dataset through SVM\n",
    "df_arrest2 = df_arrest\n",
    "df_descent2 = df_descent\n",
    "\n",
    "#subsample of dataframe\n",
    "df_arrest2 = pd.DataFrame.sample(df_arrest2, frac = SVM_SUB_SAMPLE_SIZE, random_state = 34128)\n",
    "df_descent2 = pd.DataFrame.sample(df_descent2, frac = SVM_SUB_SAMPLE_SIZE, random_state = 34128)\n",
    "\n",
    "\n",
    "print(\"The datasets have {:,} rows and {:,} columns\".format(*df_arrest2.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "\n",
    "# create variables we are more familiar with\n",
    "if 'Arrest Type Code' in df_arrest2:\n",
    "    yaS = df_arrest2['Arrest Type Code'].values # get the labels we want\n",
    "    del df_arrest2['Arrest Type Code'] # get rid of the class label\n",
    "    XaS = df_arrest2.values # use everything else to predict!\n",
    "    \n",
    "yhataS = np.zeros(yaS.shape) # we will fill this with predictions\n",
    "\n",
    "scl = StandardScaler()\n",
    "XaS = scl.fit_transform(XaS)\n",
    "\n",
    "# create cross validation iterator\n",
    "cv = StratifiedKFold(n_splits=10, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVMa(krnl):\n",
    "    for train, test in cv.split(XaS,yaS):\n",
    "        svm_clf = SVC(C=0.5, kernel=krnl, degree=3, gamma='auto').fit(XaS[train], yaS[train])\n",
    "        yhataS[test] = svm_clf.predict(XaS[test])\n",
    "\n",
    "    lb=LabelBinarizer()\n",
    "    lb.fit(yaS)\n",
    "    yaS_lb = lb.transform(yaS)\n",
    "    yhataS_lb = lb.transform(yhataS)\n",
    "    print ('AUC Score', roc_auc_score(yaS_lb, yhataS_lb, average='weighted'))\n",
    "\n",
    "    print (mt.classification_report(yaS,yhataS,digits=3))\n",
    "\n",
    "    plot_class_acc(yaS,yhataS,title=\"SVM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SVM Sigmoid Kernel\n",
    "Now, we can start our SVM analysis.  Here we train a model using a Sigmoid kernel.  The sigmoid is a special function that is guaranteed to have a y-value constrained between 0 and 1 and crosses the y-axis at x=0.  Accuracy was better than logistic regression, at 74.25%, but we can do better.  The confusion matrix shows our predicted results across the four arrest types - Felony, Misdemeanor, Infraction, and Other (in this order).  This gives us a 4x4 matrix with the accurately predicted counts along the main diagonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if RUN_SVM1:\n",
    "    krnl='sigmoid'\n",
    "    SVMa(krnl)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SVM Linear Kernel\n",
    "\n",
    "Note that linear is the simplest model to use conceptually, as it really is as simple as drawing a line between clusters of data points (or a plane in three dimensions, a cube in four, and so on).  The result is an accuracy of 76.86%, the highest yet.  In addition to the usual confusion matrix, we have also printed out the support vectors themselves and their n-value to study their shape for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if RUN_SVM2:\n",
    "    krnl='linear'\n",
    "    SVMa(krnl)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SVM Polynomial Kernel\n",
    "Next, we use the polynomial kernel, which allows us to learn a non-linear model.  We can specify the degree (i.e. the highest number of exponents of the polynomial).  In this case, we chose to go with 3.  While higher order polynomials would result in better accuracy, they would likely overfit the model, thus making it a poor predictor.\n",
    "\n",
    "Once fit, we look at the test results and find that this model results in about 81% accuracy.  This is significantly better than the accuracy results from logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if RUN_SVM3:\n",
    "    krnl='poly'\n",
    "    SVMa(krnl)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SVM Radial Basis Function Kernel\n",
    "For our final SVM model, we use RBF, which, much like sigmoid, ranges between 0 and 1.  It measures the distance between points and the support vectors, and another useful property is that RBF decreases with distance.  The result is an accuracy 81.12%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if RUN_SVM4:\n",
    "    krnl='rbf'\n",
    "    SVMa(krnl)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Stochastic Gradient Descent\n",
    "Just to see what the differences might be, we also tried a model with SGD.  Much like SVM, this also involved the use of some scaling before getting the test set prediction.  The results and confusion matrix show that accuracy with this method was as good as with SVM using and RBF kernel.  While performance in this analysis was very similar for both models, using SGD could result in better performance since it doesn't explode exponentially the way that the SVM algorithm does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Linear SVM classifier with Stochastic Descent\n",
    "if RUN_MAIN1:\n",
    "    from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "    regularize_const = 0.1\n",
    "    iterations = 5\n",
    "\n",
    "\n",
    "    for train, test in cv.split(XaS,yaS):\n",
    "        svm_clf = svm_sgd = SGDClassifier(alpha=regularize_const,\n",
    "            fit_intercept=True, l1_ratio=0.0, learning_rate='optimal',\n",
    "            loss='hinge', n_iter_no_change=iterations, n_jobs=-1, penalty='l2').fit(XaS[train], yaS[train])\n",
    "        yhataS[test] = svm_clf.predict(XaS[test])\n",
    "\n",
    "    lb=LabelBinarizer()\n",
    "    lb.fit(yaS)\n",
    "    yaS_lb = lb.transform(yaS)\n",
    "    yhataS_lb = lb.transform(yhataS)\n",
    "    print ('AUC Score', roc_auc_score(yaS_lb, yhataS_lb, average='weighted'))\n",
    "\n",
    "    print (mt.classification_report(yaS,yhataS,digits=3))\n",
    "\n",
    "    plot_class_acc(yaS,yhataS,title=\"SGD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SVM Performance - Arrest Type Code"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Table 5: SVM Performance for Arrest Type Code\n",
    "Performance metrics for each Arrest Type are listed as *Precision, Recall*.\n",
    "\n",
    "| Param | Param | Param | Param | Time | Accuracy | AUC | Felony | Misdemeanor | Infraction | Other | \n",
    "| :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- |\n",
    "| gbtree | auto | 6 | 0 | | | | | | | |\n",
    "| gblinear | auto | 6 | 0 | | | | | | | |\n",
    "| dart | auto | 6 | 0 |  | | | | | | |\n",
    "| gbtree | hist | 6 | 0 | | | | | | | |\n",
    "| gblinear | hist | 6 | 0 | | | | | | | |\n",
    "| dart | hist | 6 | 0 | | | | | | | |\n",
    "| gbtree | hist | 5 | 0 | | | | | | | |\n",
    "| gbtree | hist | 7 | 0 | | | | | | | |\n",
    "| gbtree | hist | 6 | 1 | | | | | | | |\n",
    "| gbtree | hist | 6 | 5 | | | | | | | |\n",
    "| gbtree | hist | 6 | 10 | | | | | | | |"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<a id = 'Section_7_b'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### b. Descent Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "\n",
    "# create variables we are more familiar with\n",
    "if 'Descent Code' in df_descent1:\n",
    "    yd = df_descent1['Descent Code'].values # get the labels we want\n",
    "    del df_descent1['Descent Code'] # get rid of the class label\n",
    "    Xd = df_descent1.values # use everything else to predict!\n",
    "    \n",
    "yhatd = np.zeros(yd.shape) # we will fill this with predictions\n",
    "\n",
    "scl = StandardScaler()\n",
    "Xd = scl.fit_transform(Xd)\n",
    "\n",
    "# create cross validation iterator\n",
    "cv = StratifiedKFold(n_splits=10, random_state=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### XGBoost - Descent Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KGBoost \n",
    "import pickle\n",
    "import xgboost as xgb\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, mean_squared_error, classification_report, roc_auc_score\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "def XGBoostd(boost, tree, depth, delta_step, etaparm):\n",
    "    for train, test in cv.split(Xd,yd):\n",
    "        xgb_model = xgb.XGBClassifier(booster=boost, tree_method=tree, max_depth=depth, max_delta_step=delta_step,eta=etaparm).fit(Xd[train], yd[train])\n",
    "        yhatd[test] = xgb_model.predict(Xd[test])\n",
    "    \n",
    "    lb=LabelBinarizer()\n",
    "    lb.fit(yd)\n",
    "    yd_lb = lb.transform(yd)\n",
    "    yhatd_lb = lb.transform(yhatd)\n",
    "    print ('AUC Score', roc_auc_score(yd_lb, yhatd_lb, average='weighted'))\n",
    "    \n",
    "    print (mt.classification_report(yd,yhatd,digits=3))\n",
    "                \n",
    "    plot_class_acc(yd,yhatd,title=\"XGBoost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Defaults"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Modifying Booster and Tree Method Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if RUN_MAIN2:\n",
    "    boost='gbtree'\n",
    "    tree='hist'\n",
    "    depth=6\n",
    "    delta_step=0\n",
    "    etaparm=0.3\n",
    "\n",
    "    XGBoostd(boost,tree,depth,delta_step,etaparm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if RUN_MAIN2:\n",
    "    boost='gblinear'\n",
    "    tree='hist'\n",
    "    depth=6\n",
    "    delta_step=0\n",
    "    etaparm=0.3\n",
    "\n",
    "    XGBoostd(boost,tree,depth,delta_step,etaparm)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Modifying Booster, Tree Method and Max Depth Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if RUN_MAIN2:\n",
    "    boost='gbtree'\n",
    "    tree='hist'\n",
    "    depth=5\n",
    "    delta_step=0\n",
    "    etaparm=0.3\n",
    "\n",
    "    XGBoostd(boost,tree,depth,delta_step,etaparm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if RUN_MAIN2:\n",
    "    boost='gbtree'\n",
    "    tree='hist'\n",
    "    depth=7\n",
    "    delta_step=0\n",
    "    etaparm=0.3\n",
    "\n",
    "    XGBoostd(boost,tree,depth,delta_step,etaparm)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Modifying Booster, Tree Method and Max Delta Step Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if RUN_MAIN2:\n",
    "    boost='gbtree'\n",
    "    tree='hist'\n",
    "    depth=6\n",
    "    delta_step=1\n",
    "    etaparm=0.3\n",
    "\n",
    "    XGBoostd(boost,tree,depth,delta_step,etaparm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if RUN_MAIN2:\n",
    "    boost='gbtree'\n",
    "    tree='hist'\n",
    "    depth=6\n",
    "    delta_step=5\n",
    "    etaparm=0.3\n",
    "\n",
    "    XGBoostd(boost,tree,depth,delta_step,etaparm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if RUN_MAIN2:\n",
    "    boost='gbtree'\n",
    "    tree='hist'\n",
    "    depth=6\n",
    "    delta_step=10\n",
    "    etaparm=0.3\n",
    "\n",
    "    XGBoostd(boost,tree,depth,delta_step,etaparm)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### XGBoost Performance for Descent Code"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Table 6: XGBoost Performance for Descent Code\n",
    "Performance metrics for each Descent Code are listed as *Precision, Recall*.\n",
    "\n",
    "| Booster | Tree Method | Max Depth | Max Delta Step | Time | Accuracy | AUC | Black | Hispanic | White | Other | \n",
    "| :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- |\n",
    "| gbtree | auto | 6 | 0 | | | | | | | |\n",
    "| gblinear | auto | 6 | 0 | | | | | | | |\n",
    "| dart | auto | 6 | 0 |  | | | | | | |\n",
    "| gbtree | hist | 6 | 0 | | | | | | | |\n",
    "| gblinear | hist | 6 | 0 | | | | | | | |\n",
    "| dart | hist | 6 | 0 | | | | | | | |\n",
    "| gbtree | hist | 5 | 0 | | | | | | | |\n",
    "| gbtree | hist | 7 | 0 | | | | | | | |\n",
    "| gbtree | hist | 6 | 1 | | | | | | | |\n",
    "| gbtree | hist | 6 | 5 | | | | | | | |\n",
    "| gbtree | hist | 6 | 10 | | | | | | | |"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LightGBM - Descent Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "def LightGBMd(estimators, l_rate, leaves, r_alpha, r_lambda, depth, subsample, thread, split_gain, colsample):\n",
    "    for train, test in cv.split(Xd,yd):\n",
    "        lgbm_model = lgb.LGBMClassifier(n_estimators = estimators,\n",
    "                                        objective = 'multiclass',\n",
    "                                        learning_rate = l_rate,\n",
    "                                        num_leaves = leaves,\n",
    "                                        lambda_l1 = r_alpha,\n",
    "                                        lambda_l2 = r_lambda, \n",
    "                                        maxdepth = depth,\n",
    "                                        bagging_fraction = subsample,\n",
    "                                        nthread = thread,\n",
    "                                        min_gain_to_split = split_gain,\n",
    "                                        feature_fraction = colsample,\n",
    "                                        random_state = 17).fit(Xd[train], yd[train])\n",
    "        yhatd[test] = lgbm_model.predict(Xd[test])\n",
    "    \n",
    "    lb = LabelBinarizer()\n",
    "    lb.fit(yd)\n",
    "    yd_lb = lb.transform(yd)\n",
    "    yhatd_lb = lb.transform(yhatd)\n",
    "    print ('AUC Score', roc_auc_score(yd_lb, yhatd_lb, average = 'weighted'))\n",
    "    \n",
    "    print (mt.classification_report(yd, yhatd, digits = 3))\n",
    "                \n",
    "    plot_class_acc(yd, yhatd, title = \"LightGBM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if RUN_MAIN2:\n",
    "    estimators = 100\n",
    "    objective = 'multiclass'\n",
    "    l_rate = 0.1\n",
    "    leaves = 31\n",
    "    r_alpha = 0.0\n",
    "    r_lambda = 0.0\n",
    "    depth = -1\n",
    "    subsample = 1.0\n",
    "    thread = 0\n",
    "    split_gain = 0.0\n",
    "    colsample = 1.0\n",
    "\n",
    "\n",
    "    LightGBMd(estimators, l_rate, leaves, r_alpha, r_lambda, depth, subsample, thread, split_gain, colsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if RUN_MAIN2:\n",
    "    estimators = 100\n",
    "    objective = 'multiclass'\n",
    "    l_rate = 0.1\n",
    "    leaves = 31\n",
    "    r_alpha = 0.0\n",
    "    r_lambda = 0.0\n",
    "    depth = -1\n",
    "    subsample = 1.0\n",
    "    thread = 0\n",
    "    split_gain = 0.0\n",
    "    colsample = 1.0\n",
    "\n",
    "\n",
    "    LightGBMd(estimators, l_rate, leaves, r_alpha, r_lambda, depth, subsample, thread, split_gain, colsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if RUN_MAIN2:\n",
    "    estimators = 1000\n",
    "    objective = 'multiclass'\n",
    "    l_rate = 0.05\n",
    "    leaves = 31\n",
    "    r_alpha = 0.0\n",
    "    r_lambda = 0.0\n",
    "    depth = -1\n",
    "    subsample = 1.0\n",
    "    thread = 0\n",
    "    split_gain = 0.0\n",
    "    colsample = 1.0\n",
    "\n",
    "\n",
    "    LightGBMd(estimators, l_rate, leaves, r_alpha, r_lambda, depth, subsample, thread, split_gain, colsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if RUN_MAIN2:\n",
    "    estimators = 10000\n",
    "    objective = 'multiclass'\n",
    "    l_rate = 0.05\n",
    "    leaves = 31\n",
    "    r_alpha = 0.0\n",
    "    r_lambda = 0.0\n",
    "    depth = -1\n",
    "    subsample = 1.0\n",
    "    thread = 0\n",
    "    split_gain = 0.0\n",
    "    colsample = 1.0\n",
    "\n",
    "\n",
    "    LightGBMd(estimators, l_rate, leaves, r_alpha, r_lambda, depth, subsample, thread, split_gain, colsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if RUN_MAIN2:\n",
    "    estimators = 1000\n",
    "    objective = 'multiclass'\n",
    "    l_rate = 0.05\n",
    "    leaves = 31\n",
    "    r_alpha = 0.5\n",
    "    r_lambda = 0.5\n",
    "    depth = 2\n",
    "    subsample = 1.0\n",
    "    thread = 0\n",
    "    split_gain = 0.02\n",
    "    colsample = 0.8\n",
    "\n",
    "\n",
    "    LightGBMd(estimators, l_rate, leaves, r_alpha, r_lambda, depth, subsample, thread, split_gain, colsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if RUN_MAIN2:\n",
    "    estimators = 10000\n",
    "    objective = 'multiclass'\n",
    "    l_rate = 0.05\n",
    "    leaves = 31\n",
    "    r_alpha = 0.5\n",
    "    r_lambda = 0.5\n",
    "    depth = 2\n",
    "    subsample = 1.0\n",
    "    thread = 0\n",
    "    split_gain = 0.02\n",
    "    colsample = 0.8\n",
    "\n",
    "\n",
    "    LightGBMd(estimators, l_rate, leaves, r_alpha, r_lambda, depth, subsample, thread, split_gain, colsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if RUN_MAIN2:\n",
    "    estimators = 10000\n",
    "    objective = 'multiclass'\n",
    "    l_rate = 0.05\n",
    "    leaves = 31\n",
    "    r_alpha = 0.05\n",
    "    r_lambda = 0.05\n",
    "    depth = 7\n",
    "    subsample = 1.0\n",
    "    thread = 0\n",
    "    split_gain = 0.02\n",
    "    colsample = 0.8\n",
    "\n",
    "\n",
    "    LightGBMd(estimators, l_rate, leaves, r_alpha, r_lambda, depth, subsample, thread, split_gain, colsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if RUN_MAIN2:\n",
    "    estimators = 10000\n",
    "    objective = 'multiclass'\n",
    "    l_rate = 0.05\n",
    "    leaves = 34\n",
    "    r_alpha = 0.05\n",
    "    r_lambda = 0.05\n",
    "    depth = 7\n",
    "    subsample = 1.0\n",
    "    thread = 0\n",
    "    split_gain = 0.02\n",
    "    colsample = 0.8\n",
    "\n",
    "\n",
    "    LightGBMd(estimators, l_rate, leaves, r_alpha, r_lambda, depth, subsample, thread, split_gain, colsample)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### LightGBM Performance for Descent Code"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Table 7: LightGBM Performance for Descent Code\n",
    "Performance metrics for each Descent Code are listed as *Precision, Recall*.\n",
    "\n",
    "| Param | Param | Param | Param | Time | Accuracy | AUC | Black | Hispanic | White | Other | \n",
    "| :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- |\n",
    "| gbtree | auto | 6 | 0 | | | | | | | |\n",
    "| gblinear | auto | 6 | 0 | | | | | | | |\n",
    "| dart | auto | 6 | 0 |  | | | | | | |\n",
    "| gbtree | hist | 6 | 0 | | | | | | | |\n",
    "| gblinear | hist | 6 | 0 | | | | | | | |\n",
    "| dart | hist | 6 | 0 | | | | | | | |\n",
    "| gbtree | hist | 5 | 0 | | | | | | | |\n",
    "| gbtree | hist | 7 | 0 | | | | | | | |\n",
    "| gbtree | hist | 6 | 1 | | | | | | | |\n",
    "| gbtree | hist | 6 | 5 | | | | | | | |\n",
    "| gbtree | hist | 6 | 10 | | | | | | | |"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Random Forest - Descent Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if RUN_RF3:\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "    maxAccPCARF = 0\n",
    "    optimalEstimatorsPCARF = 0\n",
    "    optimalYPCARF = []\n",
    "    optimalYhatPCARF = []\n",
    "    clReport = ''\n",
    "\n",
    "    for i in range(100, 201, 50):\n",
    "        clf_pipe = Pipeline(\n",
    "            [('PCA',PCA(n_components=50, svd_solver='randomized')),\n",
    "             ('CLF',RandomForestClassifier(n_estimators=i, n_jobs=-1))]\n",
    "        )\n",
    "\n",
    "        # now iterate through and get predictions, saved to the correct row in yhat\n",
    "        for train, test in cv.split(Xd,yd):\n",
    "            clf_pipe.fit(Xd[train],yd[train])\n",
    "            yhatd[test] = clf_pipe.predict(Xd[test])\n",
    "\n",
    "        plt.title('Confusion Matrix for {0} Estimators'.format(i), fontsize = 20)\n",
    "        plt.imshow(mt.confusion_matrix(yd, yhatd),cmap=plt.get_cmap('Greens'),aspect='auto')\n",
    "        plt.grid(False)\n",
    "        plt.show()\n",
    "\n",
    "        total_accuracy = mt.accuracy_score(yd, yhatd)\n",
    "        if total_accuracy > maxAccPCARF:\n",
    "            maxAccPCARF = total_accuracy\n",
    "            optimalEstimatorsPCARF = i\n",
    "            optimalYPCARF = yd\n",
    "            optimalYhatPCARF = yhatd\n",
    "            clReport = mt.classification_report(yd,yhatd,digits=3)\n",
    "\n",
    "        lb = LabelBinarizer()\n",
    "        lb.fit(yd)\n",
    "        yd_lb = lb.transform(yd)\n",
    "        yhatd_lb = lb.transform(yhatd)\n",
    "        print (\"Acc: \", total_accuracy)\n",
    "        print (\"AUC: \", roc_auc_score(yd_lb, yhatd_lb))\n",
    "\n",
    "    print ('Best accuracy is ', maxAccPCARF, ' with ', optimalEstimatorsPCARF, ' estimators in a random forest with PCA')\n",
    "    print(clReport)\n",
    "    plot_class_acc(optimalYPCARF,optimalYhatPCARF,title=\"Random Forest + PCA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if RUN_RF4:\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    maxAccRF = 0\n",
    "    optimalEstimatorsRF = 0\n",
    "    clReport = ''\n",
    "\n",
    "    for i in range(100, 201, 50):\n",
    "        clf = RandomForestClassifier(n_estimators=i, n_jobs=-1, oob_score=True)\n",
    "\n",
    "        # now iterate through and get predictions, saved to the correct row in yhat\n",
    "        for train, test in cv.split(Xd,yd):\n",
    "            clf.fit(Xd[train],yd[train])\n",
    "            yhatd[test] = clf.predict(Xd[test])\n",
    "\n",
    "        total_accuracy = mt.accuracy_score(yd, yhatd)\n",
    "\n",
    "        plt.title('Confusion Matrix for {0} estimators'.format(i), fontsize = 20)\n",
    "        plt.imshow(mt.confusion_matrix(yd, yhatd),cmap=plt.get_cmap('Greens'),aspect='auto')\n",
    "        plt.grid(False)\n",
    "        plt.show()\n",
    "\n",
    "        if total_accuracy > maxAccRF:\n",
    "            maxAccRF = total_accuracy\n",
    "            optimalEstimatorsRF = i\n",
    "            optimalYRF = yd\n",
    "            optimalYhatRF = yhatd\n",
    "            clReport = mt.classification_report(yd,yhatd,digits=3)\n",
    "\n",
    "        lb = LabelBinarizer()\n",
    "        lb.fit(yd)\n",
    "        yd_lb = lb.transform(yd)\n",
    "        yhatd_lb = lb.transform(yhatd)\n",
    "        print (\"Acc: \", total_accuracy)\n",
    "        print (\"AUC: \", roc_auc_score(yd_lb, yhatd_lb))\n",
    "\n",
    "    print ('Best accuracy is ', maxAccRF, ' with ', optimalEstimatorsRF, ' Estimators in a Raw Random Forest')\n",
    "    print(clReport)\n",
    "    plot_class_acc(optimalYRF,optimalYhatRF,title=\"Random Forest, Raw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Table 8: Random Forest Performance for Descent Code\n",
    "Performance metrics for each Descent Code are listed as *Precision, Recall*.\n",
    "\n",
    "| Param | Param | Param | Param | Time | Accuracy | AUC | Black | Hispanic | White | Other | \n",
    "| :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- |\n",
    "| gbtree | auto | 6 | 0 | | | | | | | |\n",
    "| gblinear | auto | 6 | 0 | | | | | | | |\n",
    "| dart | auto | 6 | 0 |  | | | | | | |\n",
    "| gbtree | hist | 6 | 0 | | | | | | | |\n",
    "| gblinear | hist | 6 | 0 | | | | | | | |\n",
    "| dart | hist | 6 | 0 | | | | | | | |\n",
    "| gbtree | hist | 5 | 0 | | | | | | | |\n",
    "| gbtree | hist | 7 | 0 | | | | | | | |\n",
    "| gbtree | hist | 6 | 1 | | | | | | | |\n",
    "| gbtree | hist | 6 | 5 | | | | | | | |\n",
    "| gbtree | hist | 6 | 10 | | | | | | | |"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SVM - Descent Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "\n",
    "# create variables we are more familiar with\n",
    "if 'Descent Code' in df_descent2:\n",
    "    ydS = df_descent2['Descent Code'].values # get the labels we want\n",
    "    del df_descent2['Descent Code'] # get rid of the class label\n",
    "    XdS = df_descent2.values # use everything else to predict!\n",
    "    \n",
    "yhatdS = np.zeros(ydS.shape) # we will fill this with predictions\n",
    "\n",
    "scl = StandardScaler()\n",
    "XdS = scl.fit_transform(XdS)\n",
    "\n",
    "# create cross validation iterator\n",
    "cv = StratifiedKFold(n_splits=10, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVMd(krnl):\n",
    "    for train, test in cv.split(XdS,ydS):\n",
    "        svm_clf = SVC(C=0.5, kernel=krnl, degree=3, gamma='auto').fit(XdS[train], ydS[train])\n",
    "        yhatdS[test] = svm_clf.predict(XdS[test])\n",
    "\n",
    "    lb=LabelBinarizer()\n",
    "    lb.fit(ydS)\n",
    "    ydS_lb = lb.transform(ydS)\n",
    "    yhatdS_lb = lb.transform(yhatdS)\n",
    "    print ('AUC Score', roc_auc_score(ydS_lb, yhatdS_lb, average='weighted'))\n",
    "\n",
    "    print (mt.classification_report(ydS,yhatdS,digits=3))\n",
    "\n",
    "    plot_class_acc(ydS,yhatdS,title=\"SVM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SVM Sigmoid Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if RUN_SVM5:\n",
    "    krnl='sigmoid'\n",
    "    SVMd(krnl)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SVM Linear Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if RUN_SVM6:\n",
    "    krnl='linear'\n",
    "    SVMd(krnl)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SVM Polynomial Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if RUN_SVM7:\n",
    "    krnl='poly'\n",
    "    SVMd(krnl)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SVM Radial Basis Function Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if RUN_SVM8:\n",
    "    krnl='rbf'\n",
    "    SVMd(krnl)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Linear SVM classifier with Stochastic Descent\n",
    "if RUN_MAIN2:\n",
    "    from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "    regularize_const = 0.1\n",
    "    iterations = 5\n",
    "\n",
    "\n",
    "    for train, test in cv.split(Xd,yd):\n",
    "        svm_clf = svm_sgd = SGDClassifier(alpha=regularize_const,\n",
    "            fit_intercept=True, l1_ratio=0.0, learning_rate='optimal',\n",
    "            loss='hinge', n_iter_no_change=iterations, n_jobs=-1, penalty='l2').fit(Xd[train], yd[train])\n",
    "        yhata[test] = svm_clf.predict(Xd[test])\n",
    "\n",
    "    lb=LabelBinarizer()\n",
    "    lb.fit(yd)\n",
    "    yd_lb = lb.transform(yd)\n",
    "    yhatd_lb = lb.transform(yhatd)\n",
    "    print ('AUC Score', roc_auc_score(yd_lb, yhatd_lb, average='weighted'))\n",
    "\n",
    "    print (mt.classification_report(yd,yhatd,digits=3))\n",
    "\n",
    "    plot_class_acc(yd,yhatd,title=\"SGD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Table 9: SVM Performance for Descent Code\n",
    "Performance metrics for each Descent Code are listed as *Precision, Recall*.\n",
    "\n",
    "| Param | Param | Param | Param | Time | Accuracy | AUC | Felony | Misdemeanor | Infraction | Other | \n",
    "| :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- |\n",
    "| gbtree | auto | 6 | 0 | | | | | | | |\n",
    "| gblinear | auto | 6 | 0 | | | | | | | |\n",
    "| dart | auto | 6 | 0 |  | | | | | | |\n",
    "| gbtree | hist | 6 | 0 | | | | | | | |\n",
    "| gblinear | hist | 6 | 0 | | | | | | | |\n",
    "| dart | hist | 6 | 0 | | | | | | | |\n",
    "| gbtree | hist | 5 | 0 | | | | | | | |\n",
    "| gbtree | hist | 7 | 0 | | | | | | | |\n",
    "| gbtree | hist | 6 | 1 | | | | | | | |\n",
    "| gbtree | hist | 6 | 5 | | | | | | | |\n",
    "| gbtree | hist | 6 | 10 | | | | | | | |"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<a id = 'Section_8'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 8. Modeling and Evaluation 4\n",
    "\n",
    "*(10 points)*\n",
    "\n",
    "*Analyze the results using your chosen method of evaluation. Use visualizations of the results to bolster the analysis. Explain any visuals and analyze why they are interesting to someone that might use this model.*"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<a id = 'Section_9'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 9. Modeling and Evaluation 5\n",
    "\n",
    "*(10 points)*\n",
    "\n",
    "*Discuss the advantages of each model for each classification task, if any. If there are not advantages, explain why. Is any model better than another? Is the difference significant with 95% confidence? Use proper statistical comparison methods. You must use statistical comparison techniques—be sure they are appropriate for your chosen method of validation as discussed in unit 7 of the course.*"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<a id = 'Section_10'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 10. Modeling and Evaluation 6\n",
    "\n",
    "*(10 points)*\n",
    "\n",
    "*Which attributes from your analysis are most important? Use proper methods discussed in class to evaluate the importance of different attributes. Discuss the results and hypothesize about why certain attributes are more important than others for a given classification task.*"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<a id = 'Section_11'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 11. Deployment\n",
    "\n",
    "*(5 points)*\n",
    "\n",
    "*How useful is your model for interested parties (i.e., the companies or organizations that might want to use it for prediction)? How would you measure the model's value if it was used by these parties? How would your deploy your model for interested parties? What other data should be collected? How often would the model need to be updated, etc.?*"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<a id = 'Section_12'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 12. Exceptional Work\n",
    "\n",
    "*(10 points)*\n",
    "\n",
    "*You have free reign to provide additional analyses. One idea: grid search parameters in a parallelized fashion and visualize the performances across attributes. Which parameters are most significant for making a good model for each classification algorithm?*"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Grid Searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if RUN_GRIDSEARCH:\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "    #knn example\n",
    "    #create parameter grid\n",
    "    knn = KNeighborsClassifier(n_neighbors = 1)\n",
    "\n",
    "    k_range = list(range(1,32,2))\n",
    "    param_grid = dict(n_neighbors = k_range)\n",
    "\n",
    "    grid = GridSearchCV(knn, param_grid, cv = 10, scoring = 'accuracy', return_train_score = False, n_jobs)\n",
    "\n",
    "    grid.fit(Xa, ya)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_GRIDSEARCH:\n",
    "    pd.DataFrame(grid.cv_results_)[['mean_test_score', 'std_test_score', 'params']]"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Grid search says ....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(k_range, )\n",
    "if RUN_GRIDSEARCH:\n",
    "    score = pd.DataFrame(grid.cv_results_)['mean_test_score']\n",
    "    plt.plot(k_range, score)\n",
    "    plt.xlabel('Value of K for KNN')\n",
    "    plt.ylabel('Mean Score Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Multiple parameter grid search\n",
    "\n",
    "passes dict of possible variables to run function (knn), can add more? limited value depending on the type of analysis being performed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multi parameter grid search\n",
    "\n",
    "if RUN_GRIDSEARCH:\n",
    "    k_range = list(range(1, 31, 2))\n",
    "    weight_options = ['uniform', 'distance']\n",
    "\n",
    "    param_grid = dict(n_neighbors = k_range, weights = weight_options)\n",
    "    print(param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if RUN_GRIDSEARCH:\n",
    "    grid = GridSearchCV(knn, param_grid, cv = 10, scoring = 'accuracy', return_train_score = False)\n",
    "    grid.fit(Xa, ya)\n",
    "    results = pd.DataFrame(grid.cv_results_)[['mean_test_score', 'std_test_score', 'params']]\n",
    "    results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#score_dist = pd.DataFrame(grid.cv_results_)['mean_test_score']\n",
    "\n",
    "if RUN_GRIDSEARCH:\n",
    "    weight_u = pd.DataFrame(grid.cv_results_)[pd.DataFrame(grid.cv_results_)['param_weights'] == 'uniform']#results from uniform parameter weights\n",
    "    weight_d = pd.DataFrame(grid.cv_results_)[pd.DataFrame(grid.cv_results_)['param_weights'] == 'distance']\n",
    "\n",
    "    plt.plot(k_range, weight_u['mean_test_score'], color = 'orange')\n",
    "    plt.plot(k_range, weight_d['mean_test_score'], color = 'blue')\n",
    "    plt.xlabel('Value of K for KNN')\n",
    "    plt.ylabel('Mean Score Accuracy')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runTime = time.time() - startTime\n",
    "seconds = int(abs(runTime % 60))\n",
    "allMinutes = int(abs(runTime / 60))\n",
    "minutes = int(abs(allMinutes % 60))\n",
    "allHours = int(abs(allMinutes / 60))\n",
    "hours = int(abs(allMinutes / 60))\n",
    "\n",
    "print (\"Notebook run time: %2d:%2d:%2d\" % (hours, minutes, seconds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}