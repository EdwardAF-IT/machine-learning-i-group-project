{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Exploratory Data Analysis\n",
    "MSDS 7331-403, Lab 1  \n",
    "*Jenna Ford, Edward Fry, Christian Nava, and Jonathan Tan* \n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from datetime import date\n",
    "import calendar\n",
    "\n",
    "df = pd.read_csv('Data\\Arrest_Data_from_2010_to_Present.csv') # read in the csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Understanding\n",
    "*(10 points)*\n",
    "\n",
    "*Describe the purpose of the data set you selected (i.e., why was this data collected in the first place?). Describe how you would define and measure the outcomes from the dataset. That is, why is this data important and how do you know if you have mined useful knowledge from the dataset? How would you measure the effectiveness of a good prediction algorithm? Be specific.*\n",
    "\n",
    "\"This dataset reflects arrest incidents in the City of Los Angeles dating back to 2010. This data is transcribed from original arrest reports that are typed on paper and therefore there may be some inaccuracies within the data. Some location fields with missing data are noted as (0.0000°, 0.0000°). Address fields are only provided to the nearest hundred block in order to maintain privacy.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding\n",
    "\n",
    "*(10 points)*  \n",
    "*Describe the meaning and type of data (scale, values, etc.) for each\n",
    "attribute in the data file.*\n",
    "\n",
    "\n",
    "The data consists of 17 attributes and their descriptions are provided by the [City of Los Angeles open data](https://data.lacity.org/A-Safe-City/Arrest-Data-from-2010-to-Present/yru6-6re4) and [Kaggle](https://www.kaggle.com/cityofLA/los-angeles-crime-arrest-data). Descriptions from these locations are displayed in Table 1 below:\n",
    "\n",
    "**Table 1: Attribute Descriptions**\n",
    "\n",
    "| Attribute | Description |\n",
    "| :--- | :--- |\n",
    "| **Report ID** | ID for the arrest |\n",
    "| **Arrest Date** | Date in MM/DD/YYYY format |\n",
    "| **Time** | In 24-hour military time |\n",
    "| **Area ID** | The LAPD has 21 Community Police Stations referred to as Geographic Areas within the department. These Geographic Areas are sequentially numbered from 1-21. |\n",
    "| **Area Name** | The 21 Geographic Areas or Patrol Divisions are also given a name designation that references a landmark or the surrounding community that it is responsible for. For example 77th Street Division is located at the intersection of South Broadway and 77th Street, serving neighborhoods in South Los Angeles. |\n",
    "| **Reporting District** | A four-digit code that represents a sub-area within a Geographic Area. All arrest records reference the \"RD\" that it occurred in for statistical comparisons. Find LAPD Reporting Districts on the LA City GeoHub at http://geohub.lacity.org/datasets/c4f83909b81d4786aa8ba8a74a4b4db1_4 |\n",
    "| **Age** | Two character numeric.|\n",
    "| **Sex Code** | F - Female; M - Male|\n",
    "| **Descent Code** | Descent Code: A - Other Asian B - Black C - Chinese D - Cambodian F - Filipino G - Guamanian H - Hispanic/Latin/Mexican I - American Indian/Alaskan Native J - Japanese K - Korean L - Laotian O - Other P - Pacific Islander S - Samoan U - Hawaiian V - Vietnamese W - White X - Unknown Z - Asian Indian |\n",
    "| **Charge Group Code** | Category of arrest charge. |\n",
    "| **Charge Group Description** | Defines the Charge Group Code provided. |\n",
    "| **Arrest Type Code** | A code to indicate the type of charge the individual was arrested for. D - Dependent F - Felony I - Infraction M - Misdemeanor O - Other |\n",
    "| **Charge** | The charge the individual was arrested for. |\n",
    "| **Charge Description** | Defines the Charge provided. |\n",
    "| **Address** | Street address of crime incident rounded to the nearest hundred block to maintain anonymity. |\n",
    "| **Cross Street** | Cross Street of rounded Address. |\n",
    "| **Location** | The location where the crime incident occurred. Actual address is omitted for confidentiality. XY coordinates reflect the nearest 100 block. |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(15 points)*  \n",
    "*Verify data quality: Explain any missing values, duplicate data, and outliers. Are those mistakes? How do you deal with these problems? Be specific.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing Values\n",
    "\n",
    "A check for missing values reveals that 56% of observations have missing data for the `Cross Street` attribute. This is a significant amount that will require closer inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df(data):\n",
    "    total = data.isnull().sum().sort_values(ascending = False)\n",
    "    percent = (data.isnull().sum()/data.isnull().count()*100).sort_values(ascending = False)\n",
    "    return pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data(df).head(17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Duplicate Data\n",
    "\n",
    "To We want to make sure we have unique observations, i.e., no two records have the same values for all attributes. This will reduce the risk of biased estimates. In this dataset a duplicate record would lead to further inspection as it is unlikely two arrests were made in the exact same location, on the same date and time, for the same charge, for two individuals of the same age, gender, and ethnic descent. A check for duplicate records verifies our dataset contains unique obervations.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicate records\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Unecessary Attributes\n",
    "\n",
    "The `Charge Group Description`, `Charge Group Code`, and `Charge Description` all provide similar information as the `Charge`. Of these four attributes, `Charge` is the only attribute that does not have missing values. We will drop `Charge Group Description`, `Charge Group Code`, and `Charge Description`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove redundandt columns or those that do not relevant information\n",
    "df.drop(['Report ID',\n",
    "         'Area ID',\n",
    "         'Location',\n",
    "         'Charge Group Description', \n",
    "         'Charge Group Code', \n",
    "         'Charge Description'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign, clean up, and split out the classes of data\n",
    "ordinalFeatures = ['Arrest Date', 'Time', 'Age']\n",
    "categoricalFeatures = ['Area Name', 'Reporting District', 'Sex Code', 'Descent Code',\n",
    "                       'Charge Group Description', 'Arrest Type Code', 'Charge', 'Charge Description',\n",
    "                       'Address', 'Cross Street', 'Location']\n",
    "df['Age'] = df['Age'].astype(np.int8)\n",
    "#df['Arrest Date'] = df['Arrest Date'].astype(np.datetime64)\n",
    "df['Reporting District'] = df['Reporting District'].astype(np.str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to cleanup the time field...it is stored like 645 instead of 06:45\n",
    "df_cleansed = df\n",
    "\n",
    "#convert float to string\n",
    "df_cleansed['Time'] = df_cleansed['Time'].astype(str) \n",
    "\n",
    "#get rid of decimals\n",
    "df_cleansed['Time'] = df_cleansed['Time'].str.split(\".\", expand=True)[0] \n",
    "\n",
    "#convert missing to 0000\n",
    "df_cleansed['Time'] = df_cleansed['Time'].replace(to_replace=\"nan\",value=\"0000\") \n",
    "\n",
    "#treat 0 as missing and convert to 0000\n",
    "df_cleansed['Time'] = df_cleansed['Time'].replace(to_replace=\"0\",value=\"0000\") \n",
    "\n",
    "#2400 is not a valid time, converting to 0001 so it isn't the same as missing\n",
    "df_cleansed['Time'] = df_cleansed['Time'].replace(to_replace=\"2400\",value=\"0001\") \n",
    "\n",
    "#split the time string to get the appropriate digits that correspond to hours and minutes\n",
    "df_cleansed['Hour'] = np.where(df_cleansed['Time'].str.len() == 4,df_cleansed['Time'].str[-4:2],np.where(df_cleansed['Time'].str.len() == 3,df_cleansed['Time'].str[-3:1],\"00\"))\n",
    "df_cleansed['Minute'] = df_cleansed['Time'].str[-2:4]\n",
    "\n",
    "#put hour and minute back together in time format\n",
    "df_cleansed['NewTime'] = pd.to_datetime(df_cleansed['Hour'] + ':' + df_cleansed['Minute'] + ':00',format='%H:%M:%S').dt.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to clean up cross street field\n",
    "\n",
    "#remove duplicate whitespaces\n",
    "df_cleansed['Cross Street'] = df_cleansed['Cross Street'].replace('\\s+',' ',regex=True)\n",
    "df_cleansed['Address'] = df_cleansed['Address'].replace('\\s+',' ',regex=True)\n",
    "\n",
    "#if all digits are numeric, nullify\n",
    "df_cleansed['Address New'] = np.where(df_cleansed[\"Address\"].str.isdigit() == True,np.nan, df_cleansed[\"Address\"])\n",
    "df_cleansed['Cross Street New'] = np.where(df_cleansed[\"Cross Street\"].str.isdigit() == True,np.nan, df_cleansed[\"Cross Street\"])\n",
    "\n",
    "df_cleansed['Address_first_word'] = df_cleansed['Address'].str.split(n=1).str[0]\n",
    "df_cleansed['Street'] = np.where(df_cleansed['Address_first_word'].str.isdigit() == True,df_cleansed['Address'].str.split(n=1).str[1],df_cleansed['Address'])\n",
    "\n",
    "df_cleansed['Cross_street_first_word'] = df_cleansed['Cross Street'].str.split(n=1).str[0]\n",
    "df_cleansed['CrossStreet'] = np.where(df_cleansed['Cross_street_first_word'].str.isdigit() == True,df_cleansed['Cross Street'].str.split(n=1).str[1],df_cleansed['Cross Street'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete columns not relevant to analysis\n",
    "df_cleansed.drop(['Time','Hour','Minute','Address','Cross Street','Address New','Cross Street New','Address_first_word','Cross_street_first_word'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>1.326626e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>3.422556e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>1.360807e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>2.300000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>3.200000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>4.500000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>9.700000e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Age\n",
       "count  1.326626e+06\n",
       "mean   3.422556e+01\n",
       "std    1.360807e+01\n",
       "min    0.000000e+00\n",
       "25%    2.300000e+01\n",
       "50%    3.200000e+01\n",
       "75%    4.500000e+01\n",
       "max    9.700000e+01"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleansed.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create year, month, day, day of week columns\n",
    "\n",
    "def findYear(date):\n",
    "    year = datetime.datetime.strptime(date, '%m/%d/%Y').year\n",
    "    return(year)\n",
    "\n",
    "df_cleansed['Year'] = df_cleansed['Arrest Date'].apply(findYear)\n",
    "\n",
    "def findMonth(date):\n",
    "    month = datetime.datetime.strptime(date, '%m/%d/%Y').month\n",
    "    return(month)\n",
    "\n",
    "df_cleansed['Month'] = df_cleansed['Arrest Date'].apply(findMonth)\n",
    "\n",
    "def findDay(date):\n",
    "    day = datetime.datetime.strptime(date, '%m/%d/%Y').day\n",
    "    return(day)\n",
    "\n",
    "df_cleansed['Day'] = df_cleansed['Arrest Date'].apply(findDay)\n",
    "\n",
    "def findDay(date): \n",
    "    day = datetime.datetime.strptime(date, '%m/%d/%Y').weekday() \n",
    "    return (calendar.day_name[day]) \n",
    "\n",
    "#df_cleansed['Day of Week'] = df_cleansed['Arrest Date'].apply(findDay)\n",
    "\n",
    "def findNDay(date): \n",
    "    day = datetime.datetime.strptime(date, '%m/%d/%Y').weekday() \n",
    "    return (day) \n",
    "\n",
    "df_cleansed['N Day of Week'] = df_cleansed['Arrest Date'].apply(findNDay)\n",
    "\n",
    "#check results\n",
    "df_cleansed\n",
    "df_cleansed.to_csv(r'Cleansed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(10 points)*   \n",
    "*Give simple, appropriate statistics (range, mode, mean, median, variance,\n",
    "counts, etc.) for the most important attributes and describe what they mean or if you found something interesting. Note: You can also use data from other sources for comparison. Explain the significance of the statistics run and why they are meaningful.*  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(15 points)*  \n",
    "*Visualize the most important attributes appropriately (at least 5 attributes). Important: Provide an interpretation for each chart. Explain for each attribute why the chosen visualization is appropriate.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(15 points)*  \n",
    "*Explore relationships between attributes: Look at the attributes via scatter plots, correlation, cross-tabulation, group-wise averages, etc. as appropriate. Explain any interesting relationships.*  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(10 points)*  \n",
    "*Identify and explain interesting relationships between features and the class you are trying to predict (i.e., relationships with variables and the target classification).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(5 points)*  \n",
    "*Are there other features that could be added to the data or created from\n",
    "existing features? Which ones?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exceptional Work\n",
    "*(10 points)*  \n",
    "• *You have free reign to provide additional analyses.  \n",
    "• One idea: implement dimensionality reduction, then visualize and interpret the results.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
