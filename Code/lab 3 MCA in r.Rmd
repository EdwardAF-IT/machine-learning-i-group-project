---
title: "Lab 3 MCA "
output: html_notebook
---

```{r}
library(dplyr)
library(ggplot2)
library(tswge)
library(lubridate)
library(tictoc)
library(knitr)
library(caret) #dummy encoding
library(FactoMineR) #mca 
library(factoextra)
library(missMDA)

library(randomForest)

#library(ggROC)
library(pROC)
```

```{r}
original_data = read.csv("D:/SMU/DS 7331 machine Learning/Semester Dataset/dataset_diabetes/diabetic_data.csv")
```

```{r}
#cleaning
df_original <- data.frame(original_data)

clean1 <- which(df_original$payer_code == '?') #remove entries with no payer code data
dfc <- df_original[-c(clean1), ]

#df0 <- dfc[-c(1, 2, 6)] #original data, drops id and numbers only
#df <- dfc[-c(1, 2, 6, 12, 19:24, 25:47)] #remove medicine vars, diagnoses, weight
#df2 <- dfc[-c(1, 2, 6, 12,19:24, 25:41, 43:47)] #leaves in insulin

#dvars <- dummyVars( " ~ .", data = df)
#df3 <- data.frame(predict(dvars, newdata = df)) #1hotencoded data

df4 <-dfc[-c(1, 2, 6, 12, 16:24, 25:47)] #remove num inpatient/outpatient/emergency

dff <- (df4)
dff[dff == 0] <- NA #change all zero to NA to play nice with MASS package?
dff[,'admission_type_id'] <- as.factor(dff[, "admission_type_id"])
dff[,'discharge_disposition_id'] <- as.factor(dff[, "discharge_disposition_id"])
dff[,'admission_source_id'] <- as.factor(dff[, "admission_source_id"])
dff[,'time_in_hospital'] <- as.factor(dff[, "time_in_hospital"])
dff[, "num_lab_procedures"] <- as.factor(dff[, "num_lab_procedures"])
dff[, "num_procedures"] <- as.factor(dff[, "num_procedures" ])
dff[, "num_medications"] <- as.factor(dff[, "num_medications"])
#dff[, "number_outpatient"] <- as.factor(dff[, "number_outpatient"])
#dff[, "number_emergency"] <- as.factor(dff[, "number_emergency" ])
#dff[, "number_inpatient"] <- as.factor(dff[, "number_inpatient"])

#remove rows with na
dff <- dff[complete.cases(dff),] 
```

After one hot encoding the data, we have 60,000 entries with 54 vars. Use MCA to reduce to ???
```{r}
#using the FactoMineR MCA function
tic()
#MCA(as.factor(df3), quanti.sup = c(1:6, 8, 15, 16), quali.sup = c(7, 9:14) ,graph = TRUE)
tab.disj.comp <- imputeMCA(dff, ncp = 2)
mca1 = MCA(dff, tab.disj = tab.disj.comp$tab.disj, graph = TRUE)
#where quanti.supp is categorical supplementary vars
#quali.supp is continous supplementary vars
toc()
```
Fairly strong central grouping, not many separate groups. 

```{r}
plot.MCA(mca1, label = "none", col.ind = 'grey', col.var = 'darkred', graph.type = "ggplot", legend =list(bty = "y", x = "topleft"))
```


```{r}
fviz_screeplot(mca1, addlabels = TRUE)
fviz_mca_biplot(mca1, ggtheme = theme_minimal())
```
Rather low inertia (amount of variation explained) per component, even the largest. 

```{r}
 #dimdesc(mca1) #dimension descriptions? 
```


```{r}
#use random forest clustering on MCA results, reserving dim1 as the predictor
#first 2 components as axes
#mca1_coord <- sample_frac(data.frame(mca1$ind$coord), 0.01, replace = FALSE)
#df.pc <- prcomp(mca1_coord[1:2], center = FALSE, scale. = FALSE)

#rf.fit <- randomForest(x = mca1_coord[1:2], y = NULL, ntree = 1000)
#hclust.rf <- hclust(as.dist(1-rf.fit$proximity), method = 'ward.d2')
#rf.cluster = cutree(hclust.rf, k = 3)
#outdated library
```

Cluster on the 5 MCA components 
```{r}
#random forest model e2

#sample 
mca1_coord <- sample_frac(data.frame(mca1$ind$coord), 0.01, replace = FALSE)
#split, reserve 0.3 of the coordinate data for testing
testing <- sample(1:(length(mca1_coord$Dim.1)), (length(mca1_coord$Dim.1)*.3))
#reserve the first column as the predictor var
rf <- randomForest(x = mca1_coord[-testing, -1], y = mca1_coord$Dim.1[-testing], 
                   ntree = 1000, mtry = 2, proximity = TRUE)
rf
```

```{r}
#predict on test set, normal randomforest
y_predicted <- predict(rf, mca1_coord[testing, -1])
df_results <- data.frame(Orig = mca1_coord$Dim.1[testing], Pred = y_predicted)
#confusionMatrix(table(df_results$Orig, df_results$Pred))

#vis
mcp1 <- ggplot(data = mca1_coord[testing,])+
  geom_line(aes(x = seq(1, length(mca1_coord$Dim.1[testing]), 1), y = mca1_coord$Dim.1[testing], col = "Actuals")) +
  geom_line(aes(x = seq(1, length(mca1_coord$Dim.1[testing]), 1), y = y_predicted, col = 'Predictions')) +
  xlab("") + ylab("MCA Dimension 1") + ggtitle("RF Predicted vs Actual MCA Dimension 1")
  

mcp1 +  scale_fill_discrete(breaks = c("Original", "Predicted"), labels = c("Original", "Predicted"))

#auc calculation
library(pROC)
#roc_obj <- roc(mca1_coord$Dim.1[testing], y_predicted)
roc_obj <- roc(controls = mca1_coord$Dim.1[testing], cases =y_predicted)
auc(roc_obj)
auc_rf <- auc(roc_obj)

#AUC viz
roc_df <- data.frame(
  TPR=rev(roc_obj$sensitivities[1:190]), 
  FPR=rev(1-roc_obj$specificities[1:190]), #hmm the 1-specificities is the inverter
  labels=roc_obj$response, 
  scores=roc_obj$predictor)

rectangle <- function(x, y, width, height, density=12, angle=-45, ...) 
  polygon(c(x,x,x+width,x+width), c(y,y+height,y+height,y), 
          density=density, angle=angle, ...)

roc_df <- transform(roc_df, 
  dFPR = c(diff(FPR), 0),
  dTPR = c(diff(TPR), 0))

plot(0:10/10, 0:10/10, type='n', xlab="FPR", ylab="TPR")
abline(h=0:10/10, col="lightblue")
abline(v=0:10/10, col="lightblue")

with(roc_df, {
  mapply(rectangle, x=FPR, y=0,   
         width=dFPR, height=TPR, col="green", lwd=2)
  mapply(rectangle, x=FPR, y=TPR, 
         width=dFPR, height=dTPR, col="blue", lwd=2)

  lines(FPR, TPR, type='b', lwd=3, col="red")
})
```

```{r}
bline[1] <- unlist(bline[1])
bline[2] <- unlist(bline[2])
ggplot(data = bline) + geom_line(aes(bline[1], bline[2]))
```

```{r}
#roc attempt 2


```

```{r}
#PAM (partitioning around medodoids)
#prox <- rf$proximity
library(cluster)
scaled_mca1_coord <- scale(mca1_coord[testing, -1]) #feed 4 columns to pam
pam.rf <- pam(scaled_mca1_coord, 3) #scaled pam predict
#pam.rf <- pam(mca1_coord[testing, -1], 3)
pred <- cbind(pam.rf$clustering, mca1_coord$Dim.1[testing]) #compare preds against column 1 actuals
#table(pred[, 2], pred[, 1])

#predictions vs actuals
v2 <- ggplot(data = mca1_coord[testing, ])+
  geom_line(aes(x = seq(1, length(testing), 1), y = scale(mca1_coord$Dim.1[testing]), col = 'Actuals'))+
  geom_line(aes(x = seq(1, length(testing), 1), y = scale(pam.rf$clustering), col = 'Predictions'))+
  ggtitle("PAM clustering vs Actual MCA1 Dim 1") + xlab("") + ylab("Dim1")
v2

#ROC calculation
roc_obj <- roc(controls = scale(mca1_coord$Dim.1[testing]), cases =scale(pam.rf$clustering))
auc(roc_obj)
auc_pam <- auc(roc_obj
               )
#roc viz
roc_df <- data.frame(
  TPR=rev(roc_obj$sensitivities[1:190]), 
  FPR=rev(1-roc_obj$specificities[1:190]), 
  labels=roc_obj$response, 
  scores=roc_obj$predictor)

rectangle <- function(x, y, width, height, density=12, angle=-45, ...) 
  polygon(c(x,x,x+width,x+width), c(y,y+height,y+height,y), 
          density=density, angle=angle, ...)

roc_df <- transform(roc_df, 
  dFPR = c(diff(FPR), 0),
  dTPR = c(diff(TPR), 0))

plot(0:10/10, 0:10/10, type='n', xlab="FPR", ylab="TPR")
abline(h=0:10/10, col="lightblue")
abline(v=0:10/10, col="lightblue")

with(roc_df, {
  mapply(rectangle, x=FPR, y=0,   
         width=dFPR, height=TPR, col="green", lwd=2)
  mapply(rectangle, x=FPR, y=TPR, 
         width=dFPR, height=dTPR, col="blue", lwd=2)

  lines(FPR, TPR, type='b', lwd=3, col="red")
})

#some sort of mismatch between the centering types
```

```{r}
library(cluster)
scaled_mca1_coord <- scale(mca1_coord[testing, -1]) #feed 4 columns
km <- kmeans(scaled_mca1_coord, 3, nstart = 100) #scaled kmeans
#km <- kmeans(mca1_coord[testing, -1], 3, nstart = 100)
#pred_km <- cbind(km$cluster, mca1_coord$Dim.1[testing])

#predictions vs actuals
v2 <- ggplot(data = mca1_coord[testing, ])+
  geom_line(aes(x = seq(1, length(testing), 1), y = scale(mca1_coord$Dim.1[testing]), col = 'Actuals'))+
  geom_line(aes(x = seq(1, length(testing), 1), y = scale(km$cluster), col = 'Predictions'))+
  ggtitle("KMeans clustering vs Actual MCA1 Dim 1") + xlab("") + ylab("Dim1")
v2

#ROC calculation
roc_obj <- roc(controls = scale(mca1_coord$Dim.1[testing]), cases =scale(km$cluster))
auc(roc_obj)
auc_km <- auc(roc_obj)
#roc viz
roc_df <- data.frame(
  TPR=rev(roc_obj$sensitivities[1:190]), 
  FPR=rev(1-roc_obj$specificities[1:190]), 
  labels=roc_obj$response, 
  scores=roc_obj$predictor)

rectangle <- function(x, y, width, height, density=12, angle=-45, ...) 
  polygon(c(x,x,x+width,x+width), c(y,y+height,y+height,y), 
          density=density, angle=angle, ...)

roc_df <- transform(roc_df, 
  dFPR = c(diff(FPR), 0),
  dTPR = c(diff(TPR), 0))

plot(0:10/10, 0:10/10, type='n', xlab="FPR", ylab="TPR")
abline(h=0:10/10, col="lightblue")
abline(v=0:10/10, col="lightblue")

with(roc_df, {
  mapply(rectangle, x=FPR, y=0,   
         width=dFPR, height=TPR, col="green", lwd=2)
  mapply(rectangle, x=FPR, y=TPR, 
         width=dFPR, height=dTPR, col="blue", lwd=2)

  lines(FPR, TPR, type='b', lwd=3, col="red")
})
```

```{r}
print("AUC Scores")
paste(auc_rf, " :Random Forest")
paste(auc_pam, " :Partitioning Around Medoids")
paste(auc_km, " :Kmeans")
```

```{r}
plotellipses(mca1,keepvar=c(1:5))
#???
```





